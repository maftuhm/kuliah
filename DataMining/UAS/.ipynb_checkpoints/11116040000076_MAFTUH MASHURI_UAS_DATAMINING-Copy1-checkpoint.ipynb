{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer as tw_tokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@agnesberti01 itu kata jata kiasan anak jaman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sadbness Klo bulanan kan happy menstruasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jalan ke warteg aja mas deket, dah gt murah, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siapin lilin dulu dah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DebatCapresJICT \\n#asingkuasaipelabuhanJICT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @agnesberti01 itu kata jata kiasan anak jaman ...\n",
       "1         @sadbness Klo bulanan kan happy menstruasi\n",
       "2  Jalan ke warteg aja mas deket, dah gt murah, k...\n",
       "3                              Siapin lilin dulu dah\n",
       "4       #DebatCapresJICT \\n#asingkuasaipelabuhanJICT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data = [] # Membuat list kosong untuk menyimpan data json perbaris\n",
    "tweets_file = open('crawling_ke_1_tgl_4April2019.txt', \"r\") # membuka file\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)  # Membaca data dalam format json dari file perbaris\n",
    "        tweets_data.append(tweet) # Menambahkan data dari file ke dalam list\n",
    "    except:\n",
    "        continue\n",
    "tweets_file.close()\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets.to_csv(\"data/export_dataframe.csv\") # Eksport dataframe ke csv\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@agnesberti01 itu kata jata kiasan anak jaman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sadbness Klo bulanan kan happy menstruasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jalan ke warteg aja mas deket, dah gt murah, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siapin lilin dulu dah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DebatCapresJICT \\n#asingkuasaipelabuhanJICT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @agnesberti01 itu kata jata kiasan anak jaman ...\n",
       "1         @sadbness Klo bulanan kan happy menstruasi\n",
       "2  Jalan ke warteg aja mas deket, dah gt murah, k...\n",
       "3                              Siapin lilin dulu dah\n",
       "4       #DebatCapresJICT \\n#asingkuasaipelabuhanJICT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/export_dataframe.csv\",error_bad_lines=False,usecols =[\"text\"])\n",
    "data[data['text'].duplicated(keep=False)].sort_values('text').head(8)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file1 = open('stopword/stopword_id.txt', \"r\").read() # Membuka file stopword bahasa indonesia dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file2 = open('stopword_en/stopwords_en.txt', \"r\").read()  # Membuka file stopword bahasa inggris dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file3 = open('stopword_noise/stopword_noise.txt', \"r\").read()  # Membuka file stopword noise dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file_all = stopword_file1 + stopword_file2 + stopword_file3  # Menggabungkan ketiga string stopword sebelumnya kedalam satu string\n",
    "stopwords = stopword_file_all.split('\\n') # Memisahkan kata dalam string yang sudah digambungkan berdasarkan baris\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = dict() # Membuat dictionary kosong untuk menyimpan kata slang dan formal sebagai key dan value\n",
    "slangwords_dataframe = pd.read_csv('slangword/colloquial-indonesian-lexicon.csv') # Membuka file csv yang berisi kata slang dan formal dan mengkonversi kedalam dataframe\n",
    "for slang, formal in zip(slangwords_dataframe['slang'], slangwords_dataframe['formal']):\n",
    "    slangwords[slang] = formal # Mapping kata slang dan formal dan memasukkan ke dalam dictionary secara berulang\n",
    "\n",
    "slangword_file = open('slangword/slangword.txt', \"r\").read() # Membuka file yang berisi kata slang dan kata formal dan mengkonversi kedalam string\n",
    "slangwords_text = slangword_file.split('\\n') # Memisahkan kata berdasarkan baris namun kata slang dan kata formal masih belum terpisah. output : (['slang:formal', ...])\n",
    "#print(slangwords_text)\n",
    "for slang in slangwords_text:\n",
    "    split_slang = slang.split(\":\") # Memisahkan semua kata slang dan kata formal berdasarkan \"titik dua (:)\"\n",
    "    slangwords[split_slang[0]] = split_slang[1] # Mapping semua kata slang dan kata formal ke dalam dictionary. Output : {'slang' : 'formal', ...}\n",
    "#print(slangwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@agnesberti01 itu kata jata kiasan anak jaman ...</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sadbness Klo bulanan kan happy menstruasi</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jalan ke warteg aja mas deket, dah gt murah, k...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, ,, dah, g...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, dah, gt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siapin lilin dulu dah</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DebatCapresJICT \\n#asingkuasaipelabuhanJICT</td>\n",
       "      <td>[#debatcapresjict, #asingkuasaipelabuhanjict]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote quote \\n\\n\" hasil tidak akan mengkhianat...</td>\n",
       "      <td>[quote, quote, \", hasil, tidak, akan, mengkhia...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[23:56] #JAKARTA Tol Tomang - Karang Tengah - ...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, tomang, -, karang...</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cengkareng, -, pl...</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cawang, -, pancor...</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, buaran, -, cipinang, #...</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @agnesberti01 itu kata jata kiasan anak jaman ...   \n",
       "1         @sadbness Klo bulanan kan happy menstruasi   \n",
       "2  Jalan ke warteg aja mas deket, dah gt murah, k...   \n",
       "3                              Siapin lilin dulu dah   \n",
       "4       #DebatCapresJICT \\n#asingkuasaipelabuhanJICT   \n",
       "5  quote quote \\n\\n\" hasil tidak akan mengkhianat...   \n",
       "6  [23:56] #JAKARTA Tol Tomang - Karang Tengah - ...   \n",
       "7  [23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...   \n",
       "8  [23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...   \n",
       "9  [23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, ,, dah, g...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4      [#debatcapresjict, #asingkuasaipelabuhanjict]   \n",
       "5  [quote, quote, \", hasil, tidak, akan, mengkhia...   \n",
       "6  [[, 23:56, ], #jakarta, tol, tomang, -, karang...   \n",
       "7  [[, 23:56, ], #jakarta, tol, cengkareng, -, pl...   \n",
       "8  [[, 23:56, ], #jakarta, tol, cawang, -, pancor...   \n",
       "9  [[, 23:56, ], #jakarta, buaran, -, cipinang, #...   \n",
       "\n",
       "                                          clean_text  \n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]  \n",
       "1             [klo, bulanan, kan, happy, menstruasi]  \n",
       "2  [jalan, ke, warteg, aja, mas, deket, dah, gt, ...  \n",
       "3                         [siapin, lilin, dulu, dah]  \n",
       "4                                                 []  \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...  \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]  \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]  \n",
       "8                            [tol, cawang, pancoran]  \n",
       "9                                 [buaran, cipinang]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized'] = list(map(lambda tweet: tw_tokenizer(strip_handles=True, reduce_len=True).tokenize(tweet.lower()), data['text'])) # Memisahkan kata dalam text berasarkan \"spasi\"\n",
    "data['clean_text'] = list(map(lambda tweet: [w for w in tweet if w.isalnum()], data['tokenized'])) # Filtering kata yang hanya berisi karakater a-z dan 0-9 (Menghapus url, hashtag, mention)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>handled_slangword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@agnesberti01 itu kata jata kiasan anak jaman ...</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, zaman, 2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sadbness Klo bulanan kan happy menstruasi</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[kalau, bulanan, kan, happy, menstruasi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jalan ke warteg aja mas deket, dah gt murah, k...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, ,, dah, g...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, dah, gt, ...</td>\n",
       "      <td>[jalan, ke, warteg, saja, mas, dekat, deh, beg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siapin lilin dulu dah</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dahulu, deh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DebatCapresJICT \\n#asingkuasaipelabuhanJICT</td>\n",
       "      <td>[#debatcapresjict, #asingkuasaipelabuhanjict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote quote \\n\\n\" hasil tidak akan mengkhianat...</td>\n",
       "      <td>[quote, quote, \", hasil, tidak, akan, mengkhia...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[23:56] #JAKARTA Tol Tomang - Karang Tengah - ...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, tomang, -, karang...</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cengkareng, -, pl...</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cawang, -, pancor...</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, buaran, -, cipinang, #...</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @agnesberti01 itu kata jata kiasan anak jaman ...   \n",
       "1         @sadbness Klo bulanan kan happy menstruasi   \n",
       "2  Jalan ke warteg aja mas deket, dah gt murah, k...   \n",
       "3                              Siapin lilin dulu dah   \n",
       "4       #DebatCapresJICT \\n#asingkuasaipelabuhanJICT   \n",
       "5  quote quote \\n\\n\" hasil tidak akan mengkhianat...   \n",
       "6  [23:56] #JAKARTA Tol Tomang - Karang Tengah - ...   \n",
       "7  [23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...   \n",
       "8  [23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...   \n",
       "9  [23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, ,, dah, g...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4      [#debatcapresjict, #asingkuasaipelabuhanjict]   \n",
       "5  [quote, quote, \", hasil, tidak, akan, mengkhia...   \n",
       "6  [[, 23:56, ], #jakarta, tol, tomang, -, karang...   \n",
       "7  [[, 23:56, ], #jakarta, tol, cengkareng, -, pl...   \n",
       "8  [[, 23:56, ], #jakarta, tol, cawang, -, pancor...   \n",
       "9  [[, 23:56, ], #jakarta, buaran, -, cipinang, #...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, dah, gt, ...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4                                                 []   \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...   \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]   \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]   \n",
       "8                            [tol, cawang, pancoran]   \n",
       "9                                 [buaran, cipinang]   \n",
       "\n",
       "                                   handled_slangword  \n",
       "0       [itu, kata, jata, kiasan, anak, zaman, 2012]  \n",
       "1           [kalau, bulanan, kan, happy, menstruasi]  \n",
       "2  [jalan, ke, warteg, saja, mas, dekat, deh, beg...  \n",
       "3                       [siapin, lilin, dahulu, deh]  \n",
       "4                                                 []  \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...  \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]  \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]  \n",
       "8                            [tol, cawang, pancoran]  \n",
       "9                                 [buaran, cipinang]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['handled_slangword'] = list(map(lambda tweet : list(map(lambda w : slangwords[w] if w in slangwords.keys() else w, tweet)), data['clean_text']))\n",
    "# Mengubah kata slang menjadi kata formal (kata slang dan kata formal yang diperoleh dari dictionary) \n",
    "#print(handled_slangword[:100])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>handled_slangword</th>\n",
       "      <th>removed_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@agnesberti01 itu kata jata kiasan anak jaman ...</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, zaman, 2012]</td>\n",
       "      <td>[jata, kiasan, zaman, 2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sadbness Klo bulanan kan happy menstruasi</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[kalau, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[bulanan, happy, menstruasi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jalan ke warteg aja mas deket, dah gt murah, k...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, ,, dah, g...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, dah, gt, ...</td>\n",
       "      <td>[jalan, ke, warteg, saja, mas, dekat, deh, beg...</td>\n",
       "      <td>[jalan, warteg, murah, tidak punya, uang, engg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siapin lilin dulu dah</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dahulu, deh]</td>\n",
       "      <td>[siapin, lilin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DebatCapresJICT \\n#asingkuasaipelabuhanJICT</td>\n",
       "      <td>[#debatcapresjict, #asingkuasaipelabuhanjict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote quote \\n\\n\" hasil tidak akan mengkhianat...</td>\n",
       "      <td>[quote, quote, \", hasil, tidak, akan, mengkhia...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "      <td>[quote, quote, hasil, mengkhianati, proses, ny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[23:56] #JAKARTA Tol Tomang - Karang Tengah - ...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, tomang, -, karang...</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "      <td>[tol, tomang, karang, tangerang, bitung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cengkareng, -, pl...</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cawang, -, pancor...</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, buaran, -, cipinang, #...</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "      <td>[buaran]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @agnesberti01 itu kata jata kiasan anak jaman ...   \n",
       "1         @sadbness Klo bulanan kan happy menstruasi   \n",
       "2  Jalan ke warteg aja mas deket, dah gt murah, k...   \n",
       "3                              Siapin lilin dulu dah   \n",
       "4       #DebatCapresJICT \\n#asingkuasaipelabuhanJICT   \n",
       "5  quote quote \\n\\n\" hasil tidak akan mengkhianat...   \n",
       "6  [23:56] #JAKARTA Tol Tomang - Karang Tengah - ...   \n",
       "7  [23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...   \n",
       "8  [23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...   \n",
       "9  [23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, ,, dah, g...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4      [#debatcapresjict, #asingkuasaipelabuhanjict]   \n",
       "5  [quote, quote, \", hasil, tidak, akan, mengkhia...   \n",
       "6  [[, 23:56, ], #jakarta, tol, tomang, -, karang...   \n",
       "7  [[, 23:56, ], #jakarta, tol, cengkareng, -, pl...   \n",
       "8  [[, 23:56, ], #jakarta, tol, cawang, -, pancor...   \n",
       "9  [[, 23:56, ], #jakarta, buaran, -, cipinang, #...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, dah, gt, ...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4                                                 []   \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...   \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]   \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]   \n",
       "8                            [tol, cawang, pancoran]   \n",
       "9                                 [buaran, cipinang]   \n",
       "\n",
       "                                   handled_slangword  \\\n",
       "0       [itu, kata, jata, kiasan, anak, zaman, 2012]   \n",
       "1           [kalau, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, saja, mas, dekat, deh, beg...   \n",
       "3                       [siapin, lilin, dahulu, deh]   \n",
       "4                                                 []   \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...   \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]   \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]   \n",
       "8                            [tol, cawang, pancoran]   \n",
       "9                                 [buaran, cipinang]   \n",
       "\n",
       "                                   removed_stopwords  \n",
       "0                        [jata, kiasan, zaman, 2012]  \n",
       "1                       [bulanan, happy, menstruasi]  \n",
       "2  [jalan, warteg, murah, tidak punya, uang, engg...  \n",
       "3                                    [siapin, lilin]  \n",
       "4                                                 []  \n",
       "5  [quote, quote, hasil, mengkhianati, proses, ny...  \n",
       "6           [tol, tomang, karang, tangerang, bitung]  \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]  \n",
       "8                            [tol, cawang, pancoran]  \n",
       "9                                           [buaran]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['removed_stopwords'] = list(map(lambda tweet : [w for w in tweet if w not in stopwords], data['handled_slangword']))\n",
    "# Filtering data dengan menghapus kata yang tidak bermakna (Stopword yang diperoleh dari file)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>handled_slangword</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>stemmed_by_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@agnesberti01 itu kata jata kiasan anak jaman ...</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, jaman, 2012]</td>\n",
       "      <td>[itu, kata, jata, kiasan, anak, zaman, 2012]</td>\n",
       "      <td>[jata, kiasan, zaman, 2012]</td>\n",
       "      <td>[jata, kias, zaman, 2012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sadbness Klo bulanan kan happy menstruasi</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[klo, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[kalau, bulanan, kan, happy, menstruasi]</td>\n",
       "      <td>[bulanan, happy, menstruasi]</td>\n",
       "      <td>[bulan, happy, menstruasi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jalan ke warteg aja mas deket, dah gt murah, k...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, ,, dah, g...</td>\n",
       "      <td>[jalan, ke, warteg, aja, mas, deket, dah, gt, ...</td>\n",
       "      <td>[jalan, ke, warteg, saja, mas, dekat, deh, beg...</td>\n",
       "      <td>[jalan, warteg, murah, tidak punya, uang, engg...</td>\n",
       "      <td>[jalan, warteg, murah, punya, uang, usah, soka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siapin lilin dulu dah</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dulu, dah]</td>\n",
       "      <td>[siapin, lilin, dahulu, deh]</td>\n",
       "      <td>[siapin, lilin]</td>\n",
       "      <td>[siapin, lilin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DebatCapresJICT \\n#asingkuasaipelabuhanJICT</td>\n",
       "      <td>[#debatcapresjict, #asingkuasaipelabuhanjict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote quote \\n\\n\" hasil tidak akan mengkhianat...</td>\n",
       "      <td>[quote, quote, \", hasil, tidak, akan, mengkhia...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "      <td>[quote, quote, hasil, tidak, akan, mengkhianat...</td>\n",
       "      <td>[quote, quote, hasil, mengkhianati, proses, ny...</td>\n",
       "      <td>[quote, quote, hasil, khianat, proses, nyata]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[23:56] #JAKARTA Tol Tomang - Karang Tengah - ...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, tomang, -, karang...</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "      <td>[tol, tomang, karang, tengah, tangerang, bitung]</td>\n",
       "      <td>[tol, tomang, karang, tangerang, bitung]</td>\n",
       "      <td>[tol, tomang, karang, tangerang, bitung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cengkareng, -, pl...</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "      <td>[tol, cengkareng, pluit, tomang, cawang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, tol, cawang, -, pancor...</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "      <td>[tol, cawang, pancoran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...</td>\n",
       "      <td>[[, 23:56, ], #jakarta, buaran, -, cipinang, #...</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "      <td>[buaran, cipinang]</td>\n",
       "      <td>[buaran]</td>\n",
       "      <td>[buaran]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @agnesberti01 itu kata jata kiasan anak jaman ...   \n",
       "1         @sadbness Klo bulanan kan happy menstruasi   \n",
       "2  Jalan ke warteg aja mas deket, dah gt murah, k...   \n",
       "3                              Siapin lilin dulu dah   \n",
       "4       #DebatCapresJICT \\n#asingkuasaipelabuhanJICT   \n",
       "5  quote quote \\n\\n\" hasil tidak akan mengkhianat...   \n",
       "6  [23:56] #JAKARTA Tol Tomang - Karang Tengah - ...   \n",
       "7  [23:56] #JAKARTA Tol Cengkareng - Pluit - Toma...   \n",
       "8  [23:56] #JAKARTA Tol Cawang - Pancoran #LANCAR...   \n",
       "9  [23:56] #JAKARTA Buaran - Cipinang #LANCAR #Ja...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, ,, dah, g...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4      [#debatcapresjict, #asingkuasaipelabuhanjict]   \n",
       "5  [quote, quote, \", hasil, tidak, akan, mengkhia...   \n",
       "6  [[, 23:56, ], #jakarta, tol, tomang, -, karang...   \n",
       "7  [[, 23:56, ], #jakarta, tol, cengkareng, -, pl...   \n",
       "8  [[, 23:56, ], #jakarta, tol, cawang, -, pancor...   \n",
       "9  [[, 23:56, ], #jakarta, buaran, -, cipinang, #...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0       [itu, kata, jata, kiasan, anak, jaman, 2012]   \n",
       "1             [klo, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, aja, mas, deket, dah, gt, ...   \n",
       "3                         [siapin, lilin, dulu, dah]   \n",
       "4                                                 []   \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...   \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]   \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]   \n",
       "8                            [tol, cawang, pancoran]   \n",
       "9                                 [buaran, cipinang]   \n",
       "\n",
       "                                   handled_slangword  \\\n",
       "0       [itu, kata, jata, kiasan, anak, zaman, 2012]   \n",
       "1           [kalau, bulanan, kan, happy, menstruasi]   \n",
       "2  [jalan, ke, warteg, saja, mas, dekat, deh, beg...   \n",
       "3                       [siapin, lilin, dahulu, deh]   \n",
       "4                                                 []   \n",
       "5  [quote, quote, hasil, tidak, akan, mengkhianat...   \n",
       "6   [tol, tomang, karang, tengah, tangerang, bitung]   \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]   \n",
       "8                            [tol, cawang, pancoran]   \n",
       "9                                 [buaran, cipinang]   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0                        [jata, kiasan, zaman, 2012]   \n",
       "1                       [bulanan, happy, menstruasi]   \n",
       "2  [jalan, warteg, murah, tidak punya, uang, engg...   \n",
       "3                                    [siapin, lilin]   \n",
       "4                                                 []   \n",
       "5  [quote, quote, hasil, mengkhianati, proses, ny...   \n",
       "6           [tol, tomang, karang, tangerang, bitung]   \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]   \n",
       "8                            [tol, cawang, pancoran]   \n",
       "9                                           [buaran]   \n",
       "\n",
       "                                    stemmed_by_spacy  \n",
       "0                          [jata, kias, zaman, 2012]  \n",
       "1                         [bulan, happy, menstruasi]  \n",
       "2  [jalan, warteg, murah, punya, uang, usah, soka...  \n",
       "3                                    [siapin, lilin]  \n",
       "4                                                 []  \n",
       "5      [quote, quote, hasil, khianat, proses, nyata]  \n",
       "6           [tol, tomang, karang, tangerang, bitung]  \n",
       "7           [tol, cengkareng, pluit, tomang, cawang]  \n",
       "8                            [tol, cawang, pancoran]  \n",
       "9                                           [buaran]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.id import Indonesian # Import modul spacy bahasa indonesia\n",
    "\n",
    "nlp = Indonesian() # memanggil objek Indonesian() pada modul spacy\n",
    "def stem_spacy(text): # Fungsi untuk mengubah kata-kata menjadi kata dasar\n",
    "    for txt in nlp(text):\n",
    "        t = txt.lemma_\n",
    "    return t\n",
    "data['stemmed_by_spacy'] = list(map(lambda tweet : list(map(lambda word: stem_spacy(word), tweet)), data['removed_stopwords']))\n",
    "#tweetsIn.to_csv(\"data/\" + str(time.time()) + \"_export_clean_text.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-44757a068fea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemmed_by_spacy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# We look at 3 the clusters generated by k-means.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcommon_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[1;32m--> 311\u001b[1;33m                     order=order, copy=copy_x)\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[1;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'M8[ns]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)\n",
    "kmeans.fit(data['stemmed_by_spacy'] )\n",
    "# We look at 3 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = data['text'].values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19996\n",
      "['diselenggarakan', 'diselipin', 'disembah', 'disenggol', 'disentuh', 'diserobot', 'disesuaikan', 'disetiap', 'dishubbulungan', 'dishubdki_jkt', 'dishubkaltara', 'disiang', 'disiapka', 'disimpen', 'disinergikan', 'disingkapkan', 'disini', 'disisi', 'disit', 'disitu', 'diskom', 'diskon', 'diskusi', 'dismove_', 'disorkan', 'disosialisasikan', 'disporaparjtg', 'diss', 'distribusi', 'disudut', 'disuguhi', 'disukai', 'disuntik', 'disuru', 'disuruh', 'disurvey', 'disusupi', 'dita_enjung', 'ditahan', 'ditahan2', 'ditambah', 'ditambah2', 'ditambahkan', 'ditandai', 'ditanggapi', 'ditanya', 'ditanyain', 'ditawarin', 'ditawarkan', 'ditegakkan', 'ditembok', 'ditempat', 'ditempel', 'ditempelin', 'ditempuh', 'ditemukan', 'ditengah2', 'diterangkan', 'diterapkan', 'diterima', 'ditertibkan', 'diteruskan', 'ditimpa', 'ditinggal', 'ditinggalkan', 'ditipu', 'dititik', 'ditjen', 'ditnggu', 'ditolak', 'ditonjokin', 'ditonton', 'ditranslate', 'ditugasi', 'ditujukan', 'ditumpangi', 'ditunda', 'ditunggu', 'diturunkan', 'diturunkannya', 'ditutup', 'ditwitter', 'ditwmeninnya', 'diucapinnya', 'diuji', 'diujun', 'diujung', 'diukur', 'diulangi', 'diulurin', 'diumur', 'diungkit', 'diuntungkan', 'diurus', 'divardha', 'divcpeydpg', 'divers', 'divhumas_polri', 'diwajibkan', 'diwakilkan']\n"
     ]
    }
   ],
   "source": [
    "word_features = vectorizer.get_feature_names()\n",
    "print(len(word_features))\n",
    "print(word_features[5000:5100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19886\n",
      "[\"''\", \"'s\", \"a'dhom\", 'aa', 'aaa', 'aaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaahh', 'aaamiin', 'aad', 'aagym', 'aakhirat', 'aam', 'aamiin', 'aan', 'aaooiisora', 'aatina', 'aatr', 'ab', 'abacod', 'abadi', 'abal', 'abang', 'abangjalu', 'abangku', 'abar', 'abba', 'abbasmahesabirru', 'abc', 'abd', 'abdelachrian', 'abdillahtoha', 'abdul', 'abduljam', 'abdullah', 'abdvik', 'abe', 'aber', 'abg', 'abgsang', 'abgyow', 'abhi', 'abi', 'abiamr', 'abidin', 'abigalih', 'abil', 'abisan', 'abisin', 'abisssss']\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "print(len(word_features2))\n",
    "print(word_features2[:50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=frozenset({'herein', 'move', 'again', 'everywhere', 'has', 'system', 'within', 'among', 'whom', 'herself', 'un', 'meanwhile', 'sixty', 'hereupon', 'top', 'were', ',', 'with', 'somewhere', ']', 'here', 'so', 'yours', 'couldnt', 'call', 'else', 'toward', 'found', 'is', 'might', 'side', 'can... 'almost', 'latter', 'themselves', 'by', 'that', 'done', 'in', 'together', 'have', 'being', 'thus'}),\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x000002258A50E9D8>, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()\n",
    "print(vectorizer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "    kmeans.fit(X3)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.savefig('elbow.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
