{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><h1>TUGAS III</h1></strong></center>\n",
    "<p style=\"margin-left:33.3333%;font-size:20px;font-weight:600;color:blue;\">PENGANTAR DATA MINING</p>\n",
    "<p style=\"margin-left:40%;font-size:20px;font-weight:600\">Nama Anggota:</p>\n",
    "<ul style=\"margin-left:28%;font-size:20px;font-weight:600\">\n",
    "    <li>Maftuh Mashuri (11160940000076)</li>\n",
    "    <li>Zahrotul Aulia (11160940000024)</li>\n",
    "    <li>Alif Tito SA (11160940000052)</li>\n",
    "    <li>Khairul Umam (11160940000073)</li>\n",
    "    <li>Fathi Syuhada (11150940000001)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer as tw_tokenizer\n",
    "from unidecode import unidecode\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian file\n",
    "<p>Membuka file dan memasukkan data tweet json dan menyimpannya kedalam variabel list <i>twetts_data</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = [] # Membuat list kosong untuk menyimpan data json perbaris\n",
    "tweets_file = open('data/dataset.txt', \"r\") # membuka file\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)  # Membaca data dalam format json dari file perbaris\n",
    "        tweets_data.append(tweet) # Menambahkan data dari file ke dalam list\n",
    "    except:\n",
    "        continue\n",
    "tweets_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Mapping data json kedalam bentuk dataframe</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-65b5b1f892ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Informasi tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id_str'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id_str'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame()\n",
    "# Informasi tweet\n",
    "tweets['created_at'] = list(map(lambda tweet: tweet['created_at'], tweets_data))\n",
    "tweets['id_str'] = list(map(lambda tweet: tweet['id_str'], tweets_data))\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['source'] = list(map(lambda tweet: tweet['source'], tweets_data))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "\n",
    "# Informasi user\n",
    "tweets['user_id'] = list(map(lambda tweet: tweet['user']['id_str'], tweets_data))\n",
    "tweets['user_name'] = list(map(lambda tweet: tweet['user']['name'], tweets_data))\n",
    "tweets['user_screen_name'] = list(map(lambda tweet: unidecode(tweet['user']['screen_name']), tweets_data))\n",
    "tweets['user_location'] = list(map(lambda tweet: tweet['user']['location'], tweets_data))\n",
    "tweets['user_url'] = list(map(lambda tweet: tweet['user']['url'], tweets_data))\n",
    "tweets['user_description'] = list(map(lambda tweet: tweet['user']['description'], tweets_data))\n",
    "tweets['user_followers_count'] = list(map(lambda tweet: tweet['user']['followers_count'], tweets_data))\n",
    "tweets['user_friends_count'] = list(map(lambda tweet: tweet['user']['friends_count'], tweets_data))\n",
    "tweets['user_favourites_count'] = list(map(lambda tweet: tweet['user']['favourites_count'], tweets_data))\n",
    "tweets['user_statuses_count'] = list(map(lambda tweet: tweet['user']['statuses_count'], tweets_data))\n",
    "tweets['user_created_at'] = list(map(lambda tweet: tweet['user']['created_at'], tweets_data))\n",
    "\n",
    "# Informasi Tempat\n",
    "tweets['address'] = list(map(lambda tweet: tweet['place']['full_name'] if tweet['place'] != None else None, tweets_data))\n",
    "tweets['country'] = list(map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweets_data))\n",
    "\n",
    "# Entities\n",
    "hashtags = list(map(lambda tweet: tweet['entities']['hashtags'] if tweet['entities'] != None else None, tweets_data))\n",
    "tweets['hashtags'] = list(map(lambda tweet : ', '.join(list(map(lambda tw : tw['text'], tweet))), hashtags))\n",
    "\n",
    "urls = list(map(lambda tweet: tweet['entities']['urls'] if tweet['entities'] != None else None, tweets_data))\n",
    "tweets['url'] = list(map(lambda tweet : ', '.join(list(map(lambda tw : tw['url'], tweet))), urls))\n",
    "\n",
    "mentions = list(map(lambda tweet: tweet['entities']['user_mentions'] if tweet['entities'] != None else None, tweets_data))\n",
    "tweets['mentions'] = list(map(lambda tweet : ', '.join(list(map(lambda tw : tw['screen_name'], tweet))), mentions))\n",
    "\n",
    "# Informasi quoted status\n",
    "tweets['quoted_status_id'] = list(map(lambda tweet : tweet['quoted_status']['id_str'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_name'] = list(map(lambda tweet : tweet['quoted_status']['user']['name'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_screen_name'] = list(map(lambda tweet : tweet['quoted_status']['user']['screen_name'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_text'] = list(map(lambda tweet : tweet['quoted_status']['text'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_quote_count'] = list(map(lambda tweet : tweet['quoted_status']['quote_count'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_reply_count'] = list(map(lambda tweet : tweet['quoted_status']['reply_count'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_retweet_count'] = list(map(lambda tweet : tweet['quoted_status']['retweet_count'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "tweets['quoted_status_favorite_count'] = list(map(lambda tweet : tweet['quoted_status']['favorite_count'] if 'quoted_status' in tweet.keys() else None, tweets_data))\n",
    "\n",
    "tweetsIn = tweets[tweets.lang == 'in']\n",
    "tweetsIn.to_csv(\"data/\" + str(time.time()) + \"_export_dataframe.csv\") # Eksport dataframe ke csv\n",
    "# tweetsIn.to_excel(\"data/\" + str(time.time()) + '_export_dataframe.xlsx') # Eksport dataframe ke excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian stopword\n",
    "<ul>\n",
    "    <li>Membuka file stopword(indonesia, inggris, noise)</li>\n",
    "    <li>kemudian menggabungkan semua kata yang ada di ketiga file sehingga menjadi satu teks panjang dan menyimpannya kedalam variabel <i>stopword_file_all</i></li>\n",
    "    <li>kemudian teks panjang di tokenize(dipisah perkata) dan menyimpannya ke dalam variabel list <i>stopwords</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file1 = open('stopword/stopword_id.txt', \"r\").read() # Membuka file stopword bahasa indonesia dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file2 = open('stopword_en/stopwords_en.txt', \"r\").read()  # Membuka file stopword bahasa inggris dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file3 = open('stopword_noise/stopword_noise.txt', \"r\").read()  # Membuka file stopword noise dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file_all = stopword_file1 + stopword_file2 + stopword_file3  # Menggabungkan ketiga string stopword sebelumnya kedalam satu string\n",
    "stopwords = stopword_file_all.split('\\n') # Memisahkan kata dalam string yang sudah digambungkan berdasarkan baris\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian slangwords\n",
    "<ul>\n",
    "    <li>Membuka file slangword(<i>colloquial-indonesian-lexicon.csv</i> dan <i>20190327_slangword.txt</i>)</li>\n",
    "    <li>File yang pertama membuka dengan modul pandas dan mengkonfersinya kedalam dataframe, kemudian menyimpan masing-masing kata kedalam variabel dictionary <i>slangwords</i></li>\n",
    "    <li>File yang kedua sama halnya pengoperasian pada file stopwords kemudian menyimpan masing-masing kata kedalam variabel dictionary <i>slangwords</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = dict() # Membuat dictionary kosong untuk menyimpan kata slang dan formal sebagai key dan value\n",
    "slangwords_dataframe = pd.read_csv('slangword/colloquial-indonesian-lexicon.csv') # Membuka file csv yang berisi kata slang dan formal dan mengkonversi kedalam dataframe\n",
    "for slang, formal in zip(slangwords_dataframe['slang'], slangwords_dataframe['formal']):\n",
    "    slangwords[slang] = formal # Mapping kata slang dan formal dan memasukkan ke dalam dictionary secara berulang\n",
    "\n",
    "slangword_file = open('slangword/slangword.txt', \"r\").read() # Membuka file yang berisi kata slang dan kata formal dan mengkonversi kedalam string\n",
    "slangwords_text = slangword_file.split('\\n') # Memisahkan kata berdasarkan baris namun kata slang dan kata formal masih belum terpisah. output : (['slang:formal', ...])\n",
    "#print(slangwords_text)\n",
    "for slang in slangwords_text:\n",
    "    split_slang = slang.split(\":\") # Memisahkan semua kata slang dan kata formal berdasarkan \"titik dua (:)\"\n",
    "    slangwords[split_slang[0]] = split_slang[1] # Mapping semua kata slang dan kata formal ke dalam dictionary. Output : {'slang' : 'formal', ...}\n",
    "#print(slangwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3. Tokenisasi dan Filtering \n",
    "<ul>\n",
    "    <li>Mapping semua tweet dan menyimpannya kedalam variabel list <i>array_text</i></li>\n",
    "    <li>Menggabungkan semua teks kedalam satu teks panjang <i>long_text</i></li>\n",
    "    <li>Memisahkan teks perkata</li>\n",
    "    <li>Menghapus simbol, ASCII, link (https: atau www.), dan kata lainnya</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_text = list(map(lambda tweet: unidecode(tweet).lower(), tweets['text'])) # Mapping semua text twitter dan memasukan ke dalam list. Output : ['teks panjang ...', 'teks panjang', ...]\n",
    "long_text = ' '.join(array_text) # Menggabungkan semua text yang berada dalam list kedalam satu text\n",
    "tokenized = tw_tokenizer().tokenize(long_text) # Memisahkan kata dalam text berasarkan \"spasi\"\n",
    "filtered_alfanumeric = [w for w in tokenized if w.isalnum()] # Filtering kata yang hanya berisi karakater a-z dan 0-9 (Menghapus url, hashtag, mention)\n",
    "print(filtered_alfanumeric[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Handling SlangWord\n",
    "<p>Mengubah kata-kata yang sudah melalui tahap <i>Stopword removal</i> dari kata slang menjadi kata formal</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_slangword = list(map(lambda w : slangwords[w] if w in slangwords.keys() else w, filtered_alfanumeric))\n",
    "# Mengubah kata slang menjadi kata formal (kata slang dan kata formal yang diperoleh dari dictionary) \n",
    "print(handled_slangword[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stopword removal\n",
    "<p>Menghapus kata-kata yang sering muncul dan tidak memiliki makna (yang, di, kan)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_stopwords = [w for w in handled_slangword if w not in stopwords] \n",
    "# Filtering data dengan menghapus kata yang tidak bermakna (Stopword yang diperoleh dari file)\n",
    "print(removed_stopwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Lemmalization\n",
    "<p>mengubah kata-kata dalam dataset tweet menjadi kata dasar dengan menggunakan modul <i>spacy</i> dan <i>sastrawi</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.) Sastrawi\n",
    "<p>Untuk data yang sangat besar, tidak disarankan menggunakan sastrawi karena membutuhkan running time yang sangat besar. Sampai laporan ini dibuat, belum bisa lemmatize kata dari dataset dengan menggunakan sastrawi</p>\n",
    "<code>stemmed_by_sastrawi = [stemmer.stem(w) for w in handled_slangword]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_sastrawi = time.time()\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # Import modul stemmer dari sastrawi\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory() # Membuat factori untuk stemmer dengan memanggi objek StemmerFactory()\n",
    "stemmer = factory.create_stemmer() # Membuat stemmer\n",
    "# stemmed_by_sastrawi = [stemmer.stem(w) for w in handled_slangword] # Mengubah kata-kata menjadi kata dasar dengan menggunakan modul sastrawi\n",
    "print(stemmer.stem(\"Melihat\"))\n",
    "end_time_sastrawi = time.time()\n",
    "\n",
    "total_time_sastrawi = end_time_sastrawi - start_time_sastrawi\n",
    "\n",
    "print(\"Hasil stem menggunakan modul Sastrawi adalah\", total_time_sastrawi, \"detik\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.) Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_spacy = time.time()\n",
    "from spacy.lang.id import Indonesian # Import modul spacy bahasa indonesia\n",
    "\n",
    "nlp = Indonesian() # memanggi objek Indonesian() pada modul spacy\n",
    "def stem_spacy(text): # Fungsi untuk mengubah kata-kata menjadi kata dasar\n",
    "    for txt in nlp(text):\n",
    "        t = txt.lemma_\n",
    "    return t\n",
    "stemmed_by_spacy = [stem_spacy(w) for w in handled_slangword]  # Mengubah kata-kata menjadi kata dasar dengan menggunakan modul spacy\n",
    "end_time_spacy = time.time()\n",
    "\n",
    "total_time_spacy = end_time_spacy - start_time_spacy\n",
    "\n",
    "print(\"Hasil stem menggunakan modul Spacy adalah\", total_time_spacy, \"detik\\n\") # Hasil running terakhir adalah 67.62633967399597 detik\n",
    "print(stemmed_by_spacy[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghitung frekuensi semua kata yang muncul\n",
    "<p>Kata yang <b>belum</b> melalui tahap 2-6 disimpan ke dalam variabel <b><i>count_words_before</i></b></p>\n",
    "<p>Kata yang <b>sudah</b> melalui tahap 2-6 disimpan ke dalam variabel <b><i>count_words_after</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter1  = Counter(tokenized) # Menghitung frekuensi muncul semua kata\n",
    "count_words_before = Counter1.items() # Menghitung kata\n",
    "#print(count_words_before)\n",
    "\n",
    "Counter2  = Counter(stemmed_by_spacy) # Menghitung frekuensi muncul semua kata\n",
    "count_words_after = Counter2.items() # Menghitung kata (Tidak termasuk kata yang tidak bermakna)\n",
    "#print(count_words_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi untuk membuat wordcloud\n",
    "<p>Fungsi tersebut untuk membuat wordcloud dan file yang berisi kata beserta frekuensinya</p>\n",
    "<p>Dengan parameter: </p>\n",
    "<ul>\n",
    "    <li><i>count_words</i>, yaitu untuk menginput dictionary <b><i>count_words_before</i></b> dan <b><i>count_words_after</i></b></li>\n",
    "    <li><i>create_file</i>, yaitu boolean untuk membuat file atau tidak</li>\n",
    "    <li><i>after</i>, yaitu boolean untuk menentukan dia sudah melalui proses 2-6 atau belum</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def create_wordcloud(count_words, create_file = False, after = True):  \n",
    "    text = \"\"\n",
    "    data_tweet = {}\n",
    "    for element in count_words:\n",
    "        text += str(element[0]) + \" \" + str(element[1]) + \"\\n\"\n",
    "        data_tweet[element[0]] = element[1]\n",
    "    if after:\n",
    "        nama_file = \"sesudah\"\n",
    "    else:\n",
    "        nama_file = \"sebelum\"\n",
    "    if create_file:\n",
    "\n",
    "        file = open(\"data/\" + str(time.time()) + \"_wordcloud_\" + nama_file + \"_filtering.txt\", \"w\").write(text) # membuat file wordcloud\n",
    "    \n",
    "    wordcloud = WordCloud(background_color = 'white', max_words=100, contour_width=2)\n",
    "\n",
    "    wordcloud.generate_from_frequencies(frequencies=data_tweet)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(\"Wordloud hasil dari dataset \" + nama_file + \" melalui proses 2-6\\n\", fontsize=40)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Menampilkan semua kata unik sebelum dilakukan proses poin 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(count_words_before, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Menampilkan semua kata unik sesudah dilakukan proses poin 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_wordcloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6965040c9dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_words_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'create_wordcloud' is not defined"
     ]
    }
   ],
   "source": [
    "create_wordcloud(count_words_after, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_user = pd.DataFrame()\n",
    "\n",
    "data_user['username'] = tweets['user_screen_name']\n",
    "data_user['nama'] = tweets['user_name']\n",
    "data_user['total_like'] = tweets['user_favourites_count']\n",
    "data_user['quoted_total_retweet'] = tweets['quoted_status_retweet_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Menampilkan Top 10 user yang paling banyak di retweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_user_sorted_by_retweet = data_user.sort_values(by=['quoted_total_retweet'], ascending=False)\n",
    "data_user_droped_duplicates_retweet = data_user_sorted_by_retweet.drop_duplicates(subset='username', keep='first')\n",
    "top_ten_retweet = data_user_droped_duplicates_retweet.head(10)\n",
    "top_ten_retweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat grafik 10 user retweet terbanyak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = top_ten_retweet.plot(kind='bar', x='username', y='quoted_total_retweet')\n",
    "plot.set_title('Top 10 user dengan retweet terbanyak', fontsize=15)\n",
    "plot.set_xlabel('Username', weight='bold', labelpad=15)\n",
    "plot.set_ylabel('Jumlah retweet', weight='bold', labelpad=15)\n",
    "\n",
    "plot.tick_params(axis='x', pad=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Menampilkan Top 10 user dengan likes paling banyak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f425d30667f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_user_sorted_by_like\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_user\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_like'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_user_droped_duplicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_user_sorted_by_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'username'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtop_ten_like\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_user_droped_duplicates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtop_ten_like\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_user' is not defined"
     ]
    }
   ],
   "source": [
    "data_user_sorted_by_like = data_user.sort_values(by=['total_like'], ascending=False)\n",
    "data_user_droped_duplicates = data_user_sorted_by_like.drop_duplicates(subset='username', keep='first')\n",
    "top_ten_like = data_user_droped_duplicates.head(10)\n",
    "top_ten_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat grafik 10 user like terbanyak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = top_ten_like.plot(kind='bar', x='username', y='total_like')\n",
    "plot.set_title('Top 10 user dengan likes terbanyak', fontsize=15)\n",
    "plot.set_xlabel('Username', weight='bold', labelpad=15)\n",
    "plot.set_ylabel('Jumlah like', weight='bold', labelpad=15)\n",
    "plot.tick_params(axis='x', pad=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
