{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><h1>TUGAS III</h1></strong></center>\n",
    "<p style=\"margin-left:33.3333%;font-size:20px;font-weight:600;color:blue;\">PENGANTAR DATA MINING</p>\n",
    "<p style=\"margin-left:40%;font-size:20px;font-weight:600\">Nama Anggota:</p>\n",
    "<ul style=\"margin-left:28%;font-size:20px;font-weight:600\">\n",
    "    <li>Maftuh Mashuri (11160940000076)</li>\n",
    "    <li>Zahrotul Aulia (11160940000024)</li>\n",
    "    <li>Alif Tito SA (11160940000052)</li>\n",
    "    <li>Khairul Umam (11160940000073)</li>\n",
    "    <li>Fathi Syuhada (11150940000001)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer as tw_tokenizer\n",
    "from unidecode import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian file\n",
    "<p>Membuka file dan memasukkan data tweet json dan menyimpannya kedalam variabel list <i>twetts_data</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = [] # Membuat list kosong untuk menyimpan data json perbaris\n",
    "tweets_file = open('data/dataset.txt', \"r\") # membuka file\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)  # Membaca data dalam format json dari file perbaris\n",
    "        tweets_data.append(tweet) # Menambahkan data dari file ke dalam list\n",
    "    except:\n",
    "        continue\n",
    "tweets_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Mapping data json kedalam bentuk dataframe</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "# Informasi tweet\n",
    "tweets['created_at'] = list(map(lambda tweet: tweet['created_at'], tweets_data))\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweetsIn = tweets[tweets.lang == 'in']\n",
    "tweetsIn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian stopword\n",
    "<ul>\n",
    "    <li>Membuka file stopword(indonesia, inggris, noise)</li>\n",
    "    <li>kemudian menggabungkan semua kata yang ada di ketiga file sehingga menjadi satu teks panjang dan menyimpannya kedalam variabel <i>stopword_file_all</i></li>\n",
    "    <li>kemudian teks panjang di tokenize(dipisah perkata) dan menyimpannya ke dalam variabel list <i>stopwords</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file1 = open('stopword/stopword_id.txt', \"r\").read() # Membuka file stopword bahasa indonesia dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file2 = open('stopword_en/stopwords_en.txt', \"r\").read()  # Membuka file stopword bahasa inggris dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file3 = open('stopword_noise/stopword_noise.txt', \"r\").read()  # Membuka file stopword noise dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file_all = stopword_file1 + stopword_file2 + stopword_file3  # Menggabungkan ketiga string stopword sebelumnya kedalam satu string\n",
    "stopwords = stopword_file_all.split('\\n') # Memisahkan kata dalam string yang sudah digambungkan berdasarkan baris\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian slangwords\n",
    "<ul>\n",
    "    <li>Membuka file slangword(<i>colloquial-indonesian-lexicon.csv</i> dan <i>20190327_slangword.txt</i>)</li>\n",
    "    <li>File yang pertama membuka dengan modul pandas dan mengkonfersinya kedalam dataframe, kemudian menyimpan masing-masing kata kedalam variabel dictionary <i>slangwords</i></li>\n",
    "    <li>File yang kedua sama halnya pengoperasian pada file stopwords kemudian menyimpan masing-masing kata kedalam variabel dictionary <i>slangwords</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = dict() # Membuat dictionary kosong untuk menyimpan kata slang dan formal sebagai key dan value\n",
    "slangwords_dataframe = pd.read_csv('slangword/colloquial-indonesian-lexicon.csv') # Membuka file csv yang berisi kata slang dan formal dan mengkonversi kedalam dataframe\n",
    "for slang, formal in zip(slangwords_dataframe['slang'], slangwords_dataframe['formal']):\n",
    "    slangwords[slang] = formal # Mapping kata slang dan formal dan memasukkan ke dalam dictionary secara berulang\n",
    "\n",
    "slangword_file = open('slangword/slangword.txt', \"r\").read() # Membuka file yang berisi kata slang dan kata formal dan mengkonversi kedalam string\n",
    "slangwords_text = slangword_file.split('\\n') # Memisahkan kata berdasarkan baris namun kata slang dan kata formal masih belum terpisah. output : (['slang:formal', ...])\n",
    "#print(slangwords_text)\n",
    "for slang in slangwords_text:\n",
    "    split_slang = slang.split(\":\") # Memisahkan semua kata slang dan kata formal berdasarkan \"titik dua (:)\"\n",
    "    slangwords[split_slang[0]] = split_slang[1] # Mapping semua kata slang dan kata formal ke dalam dictionary. Output : {'slang' : 'formal', ...}\n",
    "#print(slangwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3. Tokenisasi dan Filtering \n",
    "<ul>\n",
    "    <li>Mapping semua tweet dan menyimpannya kedalam variabel list <i>array_text</i></li>\n",
    "    <li>Menggabungkan semua teks kedalam satu teks panjang <i>long_text</i></li>\n",
    "    <li>Memisahkan teks perkata</li>\n",
    "    <li>Menghapus simbol, ASCII, link (https: atau www.), dan kata lainnya</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsIn['tokenized'] = list(map(lambda tweet: tw_tokenizer(strip_handles=True, reduce_len=True).tokenize(unidecode(tweet).lower()), tweetsIn['text'])) # Memisahkan kata dalam text berasarkan \"spasi\"\n",
    "tweetsIn['clean_text'] = list(map(lambda tweet: [w for w in tweet if w.isalnum()], tweetsIn['tokenized'])) # Filtering kata yang hanya berisi karakater a-z dan 0-9 (Menghapus url, hashtag, mention)\n",
    "tweetsIn.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Handling SlangWord\n",
    "<p>Mengubah kata-kata yang sudah melalui tahap <i>Stopword removal</i> dari kata slang menjadi kata formal</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsIn['handled_slangword'] = list(map(lambda tweet : list(map(lambda w : slangwords[w] if w in slangwords.keys() else w, tweet)), tweetsIn['clean_text']))\n",
    "# Mengubah kata slang menjadi kata formal (kata slang dan kata formal yang diperoleh dari dictionary) \n",
    "#print(handled_slangword[:100])\n",
    "tweetsIn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stopword removal\n",
    "<p>Menghapus kata-kata yang sering muncul dan tidak memiliki makna (yang, di, kan)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsIn['removed_stopwords'] = list(map(lambda tweet : [w for w in tweet if w not in stopwords], tweetsIn['handled_slangword']))\n",
    "# Filtering data dengan menghapus kata yang tidak bermakna (Stopword yang diperoleh dari file)\n",
    "tweetsIn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Lemmalization\n",
    "<p>mengubah kata-kata dalam dataset tweet menjadi kata dasar dengan menggunakan modul <i>spacy</i> dan <i>sastrawi</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian # Import modul spacy bahasa indonesia\n",
    "\n",
    "nlp = Indonesian() # memanggi objek Indonesian() pada modul spacy\n",
    "def stem_spacy(text): # Fungsi untuk mengubah kata-kata menjadi kata dasar\n",
    "    for txt in nlp(text):\n",
    "        t = txt.lemma_\n",
    "    return t\n",
    "tweetsIn['stemmed_by_spacy'] = list(map(lambda tweet : list(map(lambda word: stem_spacy(word), tweet)), tweetsIn['removed_stopwords']))\n",
    "tweetsIn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = []\n",
    "for tweet in tweetsIn['stemmed_by_spacy']:\n",
    "    wordcloud += tweet\n",
    "\n",
    "word_dict = []\n",
    "for tweet in tweetsIn['stemmed_by_spacy']:\n",
    "    word_dict.append(dict.fromkeys(wordcloud, 0))\n",
    "word_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menghitung_tf(word_dict, text):\n",
    "    tf_dict = {}\n",
    "    for word in text:\n",
    "        word_dict[word] += 1\n",
    "\n",
    "    text_count = len(text)\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count/float(text_count)\n",
    "    return tf_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
