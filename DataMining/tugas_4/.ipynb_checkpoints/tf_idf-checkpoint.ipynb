{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><h1>TUGAS III</h1></strong></center>\n",
    "<p style=\"margin-left:33.3333%;font-size:20px;font-weight:600;color:blue;\">PENGANTAR DATA MINING</p>\n",
    "<p style=\"margin-left:40%;font-size:20px;font-weight:600\">Nama Anggota:</p>\n",
    "<ul style=\"margin-left:28%;font-size:20px;font-weight:600\">\n",
    "    <li>Maftuh Mashuri (11160940000076)</li>\n",
    "    <li>Zahrotul Aulia (11160940000024)</li>\n",
    "    <li>Alif Tito SA (11160940000052)</li>\n",
    "    <li>Khairul Umam (11160940000073)</li>\n",
    "    <li>Fathi Syuhada (11150940000001)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer as tw_tokenizer\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian file\n",
    "<p>Membuka file dan memasukkan data tweet json dan menyimpannya kedalam variabel list <i>twetts_data</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelompok1 = pd.read_csv('data/kelompok1.csv')\n",
    "kelompok2_1 = pd.read_csv('data/kelompok2_1.csv')\n",
    "kelompok2_2 = pd.read_csv('data/kelompok2_2.csv')\n",
    "kelompok2_3 = pd.read_csv('data/kelompok2_3.csv')\n",
    "#kelompok3 = pd.read_csv('data/kelompok3.csv')\n",
    "kelompok4 = pd.read_csv('data/kelompok4.csv')\n",
    "#kelompok5 = pd.read_csv('data/kelompok5.csv')\n",
    "\n",
    "dataset = []\n",
    "for tweet in kelompok1['text']:\n",
    "    dataset.append(tweet)\n",
    "for tweet in kelompok2_1['text']:\n",
    "    dataset.append(tweet)\n",
    "for tweet in kelompok2_2['text']:\n",
    "    dataset.append(tweet)\n",
    "for tweet in kelompok2_3['text']:\n",
    "    dataset.append(tweet)\n",
    "# for tweet in kelompok3['text']:\n",
    "#     dataset.append(tweet)\n",
    "for tweet in kelompok4['text']:\n",
    "    dataset.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@danangtrip whahaha iya sih bener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[00:16] #JAKARTA #MACET Kalideres arah Batu Ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, makin kesini ku tahu yang mana yang mint...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                  @danangtrip whahaha iya sih bener\n",
       "1  #Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...\n",
       "2  [00:16] #JAKARTA #MACET Kalideres arah Batu Ce...\n",
       "3  [00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...\n",
       "4  Well, makin kesini ku tahu yang mana yang mint..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsIn = pd.DataFrame({'text':dataset})\n",
    "tweetsIn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian stopword\n",
    "<ul>\n",
    "    <li>Membuka file stopword(indonesia, inggris, noise)</li>\n",
    "    <li>kemudian menggabungkan semua kata yang ada di ketiga file sehingga menjadi satu teks panjang dan menyimpannya kedalam variabel <i>stopword_file_all</i></li>\n",
    "    <li>kemudian teks panjang di tokenize(dipisah perkata) dan menyimpannya ke dalam variabel list <i>stopwords</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file1 = open('stopword/stopword_id.txt', \"r\").read() # Membuka file stopword bahasa indonesia dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file2 = open('stopword_en/stopwords_en.txt', \"r\").read()  # Membuka file stopword bahasa inggris dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file3 = open('stopword_noise/stopword_noise.txt', \"r\").read()  # Membuka file stopword noise dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file_all = stopword_file1 + stopword_file2 + stopword_file3  # Menggabungkan ketiga string stopword sebelumnya kedalam satu string\n",
    "stopwords = stopword_file_all.split('\\n') # Memisahkan kata dalam string yang sudah digambungkan berdasarkan baris\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengoperasian slangwords\n",
    "<ul>\n",
    "    <li>Membuka file slangword(<i>colloquial-indonesian-lexicon.csv</i> dan <i>20190327_slangword.txt</i>)</li>\n",
    "    <li>File yang pertama membuka dengan modul pandas dan mengkonfersinya kedalam dataframe, kemudian menyimpan masing-masing kata kedalam variabel dictionary <i>slangwords</i></li>\n",
    "    <li>File yang kedua sama halnya pengoperasian pada file stopwords kemudian menyimpan masing-masing kata kedalam variabel dictionary <i>slangwords</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = dict() # Membuat dictionary kosong untuk menyimpan kata slang dan formal sebagai key dan value\n",
    "slangwords_dataframe = pd.read_csv('slangword/colloquial-indonesian-lexicon.csv') # Membuka file csv yang berisi kata slang dan formal dan mengkonversi kedalam dataframe\n",
    "for slang, formal in zip(slangwords_dataframe['slang'], slangwords_dataframe['formal']):\n",
    "    slangwords[slang] = formal # Mapping kata slang dan formal dan memasukkan ke dalam dictionary secara berulang\n",
    "\n",
    "slangword_file = open('slangword/slangword.txt', \"r\").read() # Membuka file yang berisi kata slang dan kata formal dan mengkonversi kedalam string\n",
    "slangwords_text = slangword_file.split('\\n') # Memisahkan kata berdasarkan baris namun kata slang dan kata formal masih belum terpisah. output : (['slang:formal', ...])\n",
    "#print(slangwords_text)\n",
    "for slang in slangwords_text:\n",
    "    split_slang = slang.split(\":\") # Memisahkan semua kata slang dan kata formal berdasarkan \"titik dua (:)\"\n",
    "    slangwords[split_slang[0]] = split_slang[1] # Mapping semua kata slang dan kata formal ke dalam dictionary. Output : {'slang' : 'formal', ...}\n",
    "#print(slangwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3. Tokenisasi dan Filtering \n",
    "<ul>\n",
    "    <li>Mapping semua tweet dan menyimpannya kedalam variabel list <i>array_text</i></li>\n",
    "    <li>Menggabungkan semua teks kedalam satu teks panjang <i>long_text</i></li>\n",
    "    <li>Memisahkan teks perkata</li>\n",
    "    <li>Menghapus simbol, ASCII, link (https: atau www.), dan kata lainnya</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@danangtrip whahaha iya sih bener</td>\n",
       "      <td>[whahaha, iya, sih, bener]</td>\n",
       "      <td>[whahaha, iya, sih, bener]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...</td>\n",
       "      <td>[#repost, antv_official, *, *, *, *, *, *, led...</td>\n",
       "      <td>[ledakan, mercon, besar, menghancurkan, rumah,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[00:16] #JAKARTA #MACET Kalideres arah Batu Ce...</td>\n",
       "      <td>[[, 00:16, ], #jakarta, #macet, kalideres, ara...</td>\n",
       "      <td>[kalideres, arah, batu, ceper, tangerang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...</td>\n",
       "      <td>[[, 00:10, ], #jakarta, tol, cawang, -, tmii, ...</td>\n",
       "      <td>[tol, cawang, tmii, cibubur, bogor, ciawi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, makin kesini ku tahu yang mana yang mint...</td>\n",
       "      <td>[well, ,, makin, kesini, ku, tahu, yang, mana,...</td>\n",
       "      <td>[well, makin, kesini, ku, tahu, yang, mana, ya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                  @danangtrip whahaha iya sih bener   \n",
       "1  #Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...   \n",
       "2  [00:16] #JAKARTA #MACET Kalideres arah Batu Ce...   \n",
       "3  [00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...   \n",
       "4  Well, makin kesini ku tahu yang mana yang mint...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                         [whahaha, iya, sih, bener]   \n",
       "1  [#repost, antv_official, *, *, *, *, *, *, led...   \n",
       "2  [[, 00:16, ], #jakarta, #macet, kalideres, ara...   \n",
       "3  [[, 00:10, ], #jakarta, tol, cawang, -, tmii, ...   \n",
       "4  [well, ,, makin, kesini, ku, tahu, yang, mana,...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                         [whahaha, iya, sih, bener]  \n",
       "1  [ledakan, mercon, besar, menghancurkan, rumah,...  \n",
       "2          [kalideres, arah, batu, ceper, tangerang]  \n",
       "3         [tol, cawang, tmii, cibubur, bogor, ciawi]  \n",
       "4  [well, makin, kesini, ku, tahu, yang, mana, ya...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsIn['tokenized'] = list(map(lambda tweet: tw_tokenizer(strip_handles=True, reduce_len=True).tokenize(unidecode(tweet).lower()), tweetsIn['text'])) # Memisahkan kata dalam text berasarkan \"spasi\"\n",
    "tweetsIn['clean_text'] = list(map(lambda tweet: [w for w in tweet if w.isalnum()], tweetsIn['tokenized'])) # Filtering kata yang hanya berisi karakater a-z dan 0-9 (Menghapus url, hashtag, mention)\n",
    "tweetsIn.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Handling SlangWord\n",
    "<p>Mengubah kata-kata yang sudah melalui tahap <i>Stopword removal</i> dari kata slang menjadi kata formal</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>handled_slangword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@danangtrip whahaha iya sih bener</td>\n",
       "      <td>[whahaha, iya, sih, bener]</td>\n",
       "      <td>[whahaha, iya, sih, bener]</td>\n",
       "      <td>[whahaha, iya, sih, benar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...</td>\n",
       "      <td>[#repost, antv_official, *, *, *, *, *, *, led...</td>\n",
       "      <td>[ledakan, mercon, besar, menghancurkan, rumah,...</td>\n",
       "      <td>[ledakan, mercon, besar, menghancurkan, rumah,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[00:16] #JAKARTA #MACET Kalideres arah Batu Ce...</td>\n",
       "      <td>[[, 00:16, ], #jakarta, #macet, kalideres, ara...</td>\n",
       "      <td>[kalideres, arah, batu, ceper, tangerang]</td>\n",
       "      <td>[kalideres, arah, batu, pendek, tangerang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...</td>\n",
       "      <td>[[, 00:10, ], #jakarta, tol, cawang, -, tmii, ...</td>\n",
       "      <td>[tol, cawang, tmii, cibubur, bogor, ciawi]</td>\n",
       "      <td>[tol, cawang, tmii, cibubur, bogor, ciawi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, makin kesini ku tahu yang mana yang mint...</td>\n",
       "      <td>[well, ,, makin, kesini, ku, tahu, yang, mana,...</td>\n",
       "      <td>[well, makin, kesini, ku, tahu, yang, mana, ya...</td>\n",
       "      <td>[well, makin, kesini, ku, tahu, yang, mana, ya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Honestly setelah balik ke indonesia hal yg pal...</td>\n",
       "      <td>[honestly, setelah, balik, ke, indonesia, hal,...</td>\n",
       "      <td>[honestly, setelah, balik, ke, indonesia, hal,...</td>\n",
       "      <td>[honestly, setelah, balik, ke, indonesia, hal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@detikcom Kang boong di bohongin gmn rasanya??</td>\n",
       "      <td>[kang, boong, di, bohongin, gmn, rasanya, ?, ?]</td>\n",
       "      <td>[kang, boong, di, bohongin, gmn, rasanya]</td>\n",
       "      <td>[kang, bohong, di, bohongin, bagaimana, rasanya]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dikecewain berkali-kali tapi tetep cinta sama ...</td>\n",
       "      <td>[dikecewain, berkali-kali, tapi, tetep, cinta,...</td>\n",
       "      <td>[dikecewain, tapi, tetep, cinta, sama, hehe, d...</td>\n",
       "      <td>[dikecewain, tapi, tetap, cinta, sama, hehe, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@AbdillahMakmur @9GAG hee aku jadi inget mingg...</td>\n",
       "      <td>[hee, aku, jadi, inget, minggu, kemaren, ga, s...</td>\n",
       "      <td>[hee, aku, jadi, inget, minggu, kemaren, ga, s...</td>\n",
       "      <td>[hee, aku, jadi, ingat, minggu, kemarin, tidak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lah donnaruma ngapa itu udh diganti aja</td>\n",
       "      <td>[lah, donnaruma, ngapa, itu, udh, diganti, aja]</td>\n",
       "      <td>[lah, donnaruma, ngapa, itu, udh, diganti, aja]</td>\n",
       "      <td>[lah, donnaruma, mengapa, itu, sudah, diganti,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                  @danangtrip whahaha iya sih bener   \n",
       "1  #Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...   \n",
       "2  [00:16] #JAKARTA #MACET Kalideres arah Batu Ce...   \n",
       "3  [00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...   \n",
       "4  Well, makin kesini ku tahu yang mana yang mint...   \n",
       "5  Honestly setelah balik ke indonesia hal yg pal...   \n",
       "6     @detikcom Kang boong di bohongin gmn rasanya??   \n",
       "7  Dikecewain berkali-kali tapi tetep cinta sama ...   \n",
       "8  @AbdillahMakmur @9GAG hee aku jadi inget mingg...   \n",
       "9            Lah donnaruma ngapa itu udh diganti aja   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                         [whahaha, iya, sih, bener]   \n",
       "1  [#repost, antv_official, *, *, *, *, *, *, led...   \n",
       "2  [[, 00:16, ], #jakarta, #macet, kalideres, ara...   \n",
       "3  [[, 00:10, ], #jakarta, tol, cawang, -, tmii, ...   \n",
       "4  [well, ,, makin, kesini, ku, tahu, yang, mana,...   \n",
       "5  [honestly, setelah, balik, ke, indonesia, hal,...   \n",
       "6    [kang, boong, di, bohongin, gmn, rasanya, ?, ?]   \n",
       "7  [dikecewain, berkali-kali, tapi, tetep, cinta,...   \n",
       "8  [hee, aku, jadi, inget, minggu, kemaren, ga, s...   \n",
       "9    [lah, donnaruma, ngapa, itu, udh, diganti, aja]   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                         [whahaha, iya, sih, bener]   \n",
       "1  [ledakan, mercon, besar, menghancurkan, rumah,...   \n",
       "2          [kalideres, arah, batu, ceper, tangerang]   \n",
       "3         [tol, cawang, tmii, cibubur, bogor, ciawi]   \n",
       "4  [well, makin, kesini, ku, tahu, yang, mana, ya...   \n",
       "5  [honestly, setelah, balik, ke, indonesia, hal,...   \n",
       "6          [kang, boong, di, bohongin, gmn, rasanya]   \n",
       "7  [dikecewain, tapi, tetep, cinta, sama, hehe, d...   \n",
       "8  [hee, aku, jadi, inget, minggu, kemaren, ga, s...   \n",
       "9    [lah, donnaruma, ngapa, itu, udh, diganti, aja]   \n",
       "\n",
       "                                   handled_slangword  \n",
       "0                         [whahaha, iya, sih, benar]  \n",
       "1  [ledakan, mercon, besar, menghancurkan, rumah,...  \n",
       "2         [kalideres, arah, batu, pendek, tangerang]  \n",
       "3         [tol, cawang, tmii, cibubur, bogor, ciawi]  \n",
       "4  [well, makin, kesini, ku, tahu, yang, mana, ya...  \n",
       "5  [honestly, setelah, balik, ke, indonesia, hal,...  \n",
       "6   [kang, bohong, di, bohongin, bagaimana, rasanya]  \n",
       "7  [dikecewain, tapi, tetap, cinta, sama, hehe, d...  \n",
       "8  [hee, aku, jadi, ingat, minggu, kemarin, tidak...  \n",
       "9  [lah, donnaruma, mengapa, itu, sudah, diganti,...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsIn['handled_slangword'] = list(map(lambda tweet : list(map(lambda w : slangwords[w] if w in slangwords.keys() else w, tweet)), tweetsIn['clean_text']))\n",
    "# Mengubah kata slang menjadi kata formal (kata slang dan kata formal yang diperoleh dari dictionary) \n",
    "#print(handled_slangword[:100])\n",
    "tweetsIn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stopword removal\n",
    "<p>Menghapus kata-kata yang sering muncul dan tidak memiliki makna (yang, di, kan)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>handled_slangword</th>\n",
       "      <th>removed_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@danangtrip whahaha iya sih bener</td>\n",
       "      <td>[whahaha, iya, sih, bener]</td>\n",
       "      <td>[whahaha, iya, sih, bener]</td>\n",
       "      <td>[whahaha, iya, sih, benar]</td>\n",
       "      <td>[whahaha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...</td>\n",
       "      <td>[#repost, antv_official, *, *, *, *, *, *, led...</td>\n",
       "      <td>[ledakan, mercon, besar, menghancurkan, rumah,...</td>\n",
       "      <td>[ledakan, mercon, besar, menghancurkan, rumah,...</td>\n",
       "      <td>[ledakan, mercon, menghancurkan, rumah, nanang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[00:16] #JAKARTA #MACET Kalideres arah Batu Ce...</td>\n",
       "      <td>[[, 00:16, ], #jakarta, #macet, kalideres, ara...</td>\n",
       "      <td>[kalideres, arah, batu, ceper, tangerang]</td>\n",
       "      <td>[kalideres, arah, batu, pendek, tangerang]</td>\n",
       "      <td>[kalideres, batu, pendek, tangerang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...</td>\n",
       "      <td>[[, 00:10, ], #jakarta, tol, cawang, -, tmii, ...</td>\n",
       "      <td>[tol, cawang, tmii, cibubur, bogor, ciawi]</td>\n",
       "      <td>[tol, cawang, tmii, cibubur, bogor, ciawi]</td>\n",
       "      <td>[tol, cawang, tmii, bogor, ciawi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, makin kesini ku tahu yang mana yang mint...</td>\n",
       "      <td>[well, ,, makin, kesini, ku, tahu, yang, mana,...</td>\n",
       "      <td>[well, makin, kesini, ku, tahu, yang, mana, ya...</td>\n",
       "      <td>[well, makin, kesini, ku, tahu, yang, mana, ya...</td>\n",
       "      <td>[kesini, ku, tolong, manfaatin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Honestly setelah balik ke indonesia hal yg pal...</td>\n",
       "      <td>[honestly, setelah, balik, ke, indonesia, hal,...</td>\n",
       "      <td>[honestly, setelah, balik, ke, indonesia, hal,...</td>\n",
       "      <td>[honestly, setelah, balik, ke, indonesia, hal,...</td>\n",
       "      <td>[honestly, berasa, beda, cuaca, astagah, kerin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@detikcom Kang boong di bohongin gmn rasanya??</td>\n",
       "      <td>[kang, boong, di, bohongin, gmn, rasanya, ?, ?]</td>\n",
       "      <td>[kang, boong, di, bohongin, gmn, rasanya]</td>\n",
       "      <td>[kang, bohong, di, bohongin, bagaimana, rasanya]</td>\n",
       "      <td>[kang, bohong, bohongin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dikecewain berkali-kali tapi tetep cinta sama ...</td>\n",
       "      <td>[dikecewain, berkali-kali, tapi, tetep, cinta,...</td>\n",
       "      <td>[dikecewain, tapi, tetep, cinta, sama, hehe, d...</td>\n",
       "      <td>[dikecewain, tapi, tetap, cinta, sama, hehe, d...</td>\n",
       "      <td>[dikecewain, cinta, dasar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@AbdillahMakmur @9GAG hee aku jadi inget mingg...</td>\n",
       "      <td>[hee, aku, jadi, inget, minggu, kemaren, ga, s...</td>\n",
       "      <td>[hee, aku, jadi, inget, minggu, kemaren, ga, s...</td>\n",
       "      <td>[hee, aku, jadi, ingat, minggu, kemarin, tidak...</td>\n",
       "      <td>[hee, kemarin, sengaja, habis, menginjak, koch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lah donnaruma ngapa itu udh diganti aja</td>\n",
       "      <td>[lah, donnaruma, ngapa, itu, udh, diganti, aja]</td>\n",
       "      <td>[lah, donnaruma, ngapa, itu, udh, diganti, aja]</td>\n",
       "      <td>[lah, donnaruma, mengapa, itu, sudah, diganti,...</td>\n",
       "      <td>[donnaruma, diganti]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                  @danangtrip whahaha iya sih bener   \n",
       "1  #Repost antv_official\\r\\n• • • • • •\\r\\nLedaka...   \n",
       "2  [00:16] #JAKARTA #MACET Kalideres arah Batu Ce...   \n",
       "3  [00:10] #JAKARTA Tol Cawang - TMII - Cibubur -...   \n",
       "4  Well, makin kesini ku tahu yang mana yang mint...   \n",
       "5  Honestly setelah balik ke indonesia hal yg pal...   \n",
       "6     @detikcom Kang boong di bohongin gmn rasanya??   \n",
       "7  Dikecewain berkali-kali tapi tetep cinta sama ...   \n",
       "8  @AbdillahMakmur @9GAG hee aku jadi inget mingg...   \n",
       "9            Lah donnaruma ngapa itu udh diganti aja   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                         [whahaha, iya, sih, bener]   \n",
       "1  [#repost, antv_official, *, *, *, *, *, *, led...   \n",
       "2  [[, 00:16, ], #jakarta, #macet, kalideres, ara...   \n",
       "3  [[, 00:10, ], #jakarta, tol, cawang, -, tmii, ...   \n",
       "4  [well, ,, makin, kesini, ku, tahu, yang, mana,...   \n",
       "5  [honestly, setelah, balik, ke, indonesia, hal,...   \n",
       "6    [kang, boong, di, bohongin, gmn, rasanya, ?, ?]   \n",
       "7  [dikecewain, berkali-kali, tapi, tetep, cinta,...   \n",
       "8  [hee, aku, jadi, inget, minggu, kemaren, ga, s...   \n",
       "9    [lah, donnaruma, ngapa, itu, udh, diganti, aja]   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                         [whahaha, iya, sih, bener]   \n",
       "1  [ledakan, mercon, besar, menghancurkan, rumah,...   \n",
       "2          [kalideres, arah, batu, ceper, tangerang]   \n",
       "3         [tol, cawang, tmii, cibubur, bogor, ciawi]   \n",
       "4  [well, makin, kesini, ku, tahu, yang, mana, ya...   \n",
       "5  [honestly, setelah, balik, ke, indonesia, hal,...   \n",
       "6          [kang, boong, di, bohongin, gmn, rasanya]   \n",
       "7  [dikecewain, tapi, tetep, cinta, sama, hehe, d...   \n",
       "8  [hee, aku, jadi, inget, minggu, kemaren, ga, s...   \n",
       "9    [lah, donnaruma, ngapa, itu, udh, diganti, aja]   \n",
       "\n",
       "                                   handled_slangword  \\\n",
       "0                         [whahaha, iya, sih, benar]   \n",
       "1  [ledakan, mercon, besar, menghancurkan, rumah,...   \n",
       "2         [kalideres, arah, batu, pendek, tangerang]   \n",
       "3         [tol, cawang, tmii, cibubur, bogor, ciawi]   \n",
       "4  [well, makin, kesini, ku, tahu, yang, mana, ya...   \n",
       "5  [honestly, setelah, balik, ke, indonesia, hal,...   \n",
       "6   [kang, bohong, di, bohongin, bagaimana, rasanya]   \n",
       "7  [dikecewain, tapi, tetap, cinta, sama, hehe, d...   \n",
       "8  [hee, aku, jadi, ingat, minggu, kemarin, tidak...   \n",
       "9  [lah, donnaruma, mengapa, itu, sudah, diganti,...   \n",
       "\n",
       "                                   removed_stopwords  \n",
       "0                                          [whahaha]  \n",
       "1  [ledakan, mercon, menghancurkan, rumah, nanang...  \n",
       "2               [kalideres, batu, pendek, tangerang]  \n",
       "3                  [tol, cawang, tmii, bogor, ciawi]  \n",
       "4                    [kesini, ku, tolong, manfaatin]  \n",
       "5  [honestly, berasa, beda, cuaca, astagah, kerin...  \n",
       "6                           [kang, bohong, bohongin]  \n",
       "7                         [dikecewain, cinta, dasar]  \n",
       "8  [hee, kemarin, sengaja, habis, menginjak, koch...  \n",
       "9                               [donnaruma, diganti]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsIn['removed_stopwords'] = list(map(lambda tweet : [w for w in tweet if w not in stopwords], tweetsIn['handled_slangword']))\n",
    "# Filtering data dengan menghapus kata yang tidak bermakna (Stopword yang diperoleh dari file)\n",
    "tweetsIn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Lemmalization\n",
    "<p>mengubah kata-kata dalam dataset tweet menjadi kata dasar dengan menggunakan modul <i>spacy</i> dan <i>sastrawi</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian # Import modul spacy bahasa indonesia\n",
    "\n",
    "nlp = Indonesian() # memanggi objek Indonesian() pada modul spacy\n",
    "def stem_spacy(text): # Fungsi untuk mengubah kata-kata menjadi kata dasar\n",
    "    for txt in nlp(text):\n",
    "        t = txt.lemma_\n",
    "    return t\n",
    "tweetsIn['stemmed_by_spacy'] = list(map(lambda tweet : list(map(lambda word: stem_spacy(word), tweet)), tweetsIn['removed_stopwords']))\n",
    "#tweetsIn.to_csv(\"data/\" + str(time.time()) + \"_export_clean_text.csv\")\n",
    "tweetsIn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfigurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInreset = tweetsIn.reset_index()\n",
    "DATA = tweetsInreset['stemmed_by_spacy']\n",
    "N = 5 # Banyaknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat list term atau kumpulan kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term(text_list, total_text = 0):\n",
    "    wordcloud = []\n",
    "    if total_text == 0:\n",
    "        total_text = len(text_list)\n",
    "\n",
    "    for word in text_list[:total_text]:\n",
    "        wordcloud += word\n",
    "\n",
    "    term = list(dict.fromkeys(wordcloud))\n",
    "    return term\n",
    "\n",
    "print(create_term(DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf(text_list, total_text= 0):\n",
    "    if total_text == 0:\n",
    "        total_text = len(text_list)\n",
    "    term = create_term(text_list, total_text)\n",
    "    tf = dict()\n",
    "    for i in range(total_text):\n",
    "        name = 'tf_text_' + str(i+1)\n",
    "        tf[name] = list(map(lambda word: text_list[i].count(word), term))\n",
    "    return tf\n",
    "\n",
    "tf = create_tf(DATA)\n",
    "# tf = create_tf(DATA, N)\n",
    "\n",
    "pd.DataFrame(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idf(text_list, total_text = 0):\n",
    "    if total_text == 0:\n",
    "        total_text = len(text_list)\n",
    "    idf = []\n",
    "    term = create_term(text_list, total_text)\n",
    "    tf = create_tf(text_list, total_text)\n",
    "    total_words = len(term)\n",
    "\n",
    "    for i in range(total_words):\n",
    "        val = 0\n",
    "        for j in range(total_text):\n",
    "            name = 'tf_text_' + str(j+1)\n",
    "            if float(tf[name][i]) > 0:\n",
    "                val += 1\n",
    "        idf.append(math.log10(total_text / float(val)))\n",
    "    return idf\n",
    "\n",
    "idf = create_idf(DATA)\n",
    "# tf = create_idf(DATA, N)\n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf(text_list, total_text = 0):\n",
    "    if total_text == 0:\n",
    "        total_text = len(text_list)\n",
    "        \n",
    "    tf_idf = dict()\n",
    "    term = create_term(text_list, total_text)\n",
    "    tf = create_tf(text_list, total_text)\n",
    "    idf = create_idf(text_list, total_text)\n",
    "\n",
    "    total_words = len(term)\n",
    "\n",
    "    for i in range(total_text):\n",
    "        name_tf = 'tf_text_' + str(i+1)\n",
    "        name_tf_idf = 'tf_idf_text_' + str(i+1)\n",
    "        list_tf_idf = []\n",
    "        for j in  range(total_words):\n",
    "            temp = 0\n",
    "            temp = tf[name_tf][j] * idf[j]\n",
    "            list_tf_idf.append(temp)\n",
    "        tf_idf[name_tf_idf] = list_tf_idf\n",
    "        \n",
    "    return tf_idf\n",
    "\n",
    "tf_idf = create_tf_idf(DATA)\n",
    "# tf = create_tf_idf(DATA, N)\n",
    "\n",
    "pd.DataFrame(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF, IDF, dan TF-IDF dikumpukan ke satu dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {**tf, 'idf' : idf, **tf_idf}\n",
    "data_tf_idf = pd.DataFrame(data)\n",
    "data_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
