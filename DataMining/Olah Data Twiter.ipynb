{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operasi dimulai...\n",
      "Jumlah data tweets : 7083\n",
      "Jumlah data tweets bahasa Indonesia : 5365\n",
      "[('2', 386), ('jakarta', 191), ('pagi', 185), ('allah', 167), ('tol', 147), ('banget', 137), ('1', 121), ('jam', 111), ('selamat', 110), ('indonesia', 109), ('tidur', 102), ('beli', 88), ('pakai', 84), ('suka', 80), ('sms', 80), ('6287788719', 77), ('668', 77), ('2019', 75), ('cawang', 74), ('anak', 72), ('imam', 72), ('semoga', 71), ('ku', 68), ('mahdi', 68), ('5', 67), ('kasih', 66), ('jalan', 59), ('hati', 58), ('amin', 58), ('sayang', 58), ('3', 58), ('100', 57), ('april', 56), ('lihat', 56), ('kerja', 55), ('asli', 55), ('rumah', 54), ('cepat', 54), ('malam', 52), ('terima', 52), ('masuk', 50), ('kemarin', 50), ('makan', 49), ('lupa', 49), ('2013', 49), ('main', 48), ('yuk', 47), ('coba', 47), ('4', 47), ('kamis', 46), ('manusia', 45), ('foto', 45), ('membantu', 45), ('rambut', 45), ('morning', 45), ('semangat', 43), ('keren', 42), ('manfaat', 42), ('2016', 42), ('krim', 42), ('open', 41), ('salah', 41), ('wak', 41), ('tangerang', 40), ('kawan', 40), ('menonton', 40), ('doyok', 40), ('jambang', 40), ('uang', 39), ('teman', 39), ('bilang', 39), ('habis', 38), ('besok', 38), ('twitter', 37), ('2015', 37), ('pe', 37), ('takut', 36), ('enak', 36), ('sakit', 36), ('daya', 36), ('dm', 35), ('video', 34), ('wkwkwk', 34), ('langsung', 34), ('wanita', 33), ('good', 33), ('po', 33), ('pembelian', 33), ('mempercepat', 33), ('maaf', 32), ('lucu', 32), ('hidup', 32), ('alhamdulillah', 32), ('bangun', 32), ('bayan', 32), ('tolong', 31), ('kakak', 31), ('day', 31), ('2014', 31), ('dunia', 31)]\n",
      "Operasi selesai\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer as tw_tokenizer\n",
    "from collections import Counter\n",
    "from unidecode import unidecode\n",
    "\n",
    "print(\"Operasi dimulai...\")\n",
    "tweets_data = [] # Membuat list kosong untuk menyimpan data json perbaris\n",
    "tweets_file = open('crawling_ke_1_tgl_4April2019.txt', \"r\") # membuka file\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)  # Membaca data dalam format json dari file perbaris\n",
    "        tweets_data.append(tweet) # Menambahkan data dari file ke dalam list\n",
    "    except:\n",
    "        continue\n",
    "tweets_file.close()\n",
    "\n",
    "stopword_file1 = open('TUGAS_3/stopword/20190327_stopword_id.txt', \"r\").read() # Membuka file stopword bahasa indonesia dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file2 = open('TUGAS_3/stopword_en/20190327_stopwords_en.txt', \"r\").read()  # Membuka file stopword bahasa inggris dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file3 = open('TUGAS_3/stopword_noise/20190327_stopword_noise.txt', \"r\").read()  # Membuka file stopword noise dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file_all = stopword_file1 + stopword_file2 + stopword_file3  # Menggabungkan ketiga string stopword sebelumnya kedalam satu string\n",
    "stopwords = stopword_file_all.split('\\n') # Memisahkan kata dalam string yang sudah digambungkan berdasarkan baris\n",
    "\n",
    "slangwords = dict() # Membuat dictionary kosong untuk menyimpan kata slang dan formal sebagai key dan value\n",
    "slangwords_dataframe = pd.read_csv('colloquial-indonesian-lexicon.csv') # Membuka file csv yang berisi kata slang dan formal dan mengkonversi kedalam dataframe\n",
    "for slang, formal in zip(slangwords_dataframe['slang'], slangwords_dataframe['formal']):\n",
    "    slangwords[slang] = formal # Mapping kata slang dan formal dan memasukkan ke dalam dictionary secara berulang\n",
    "\n",
    "slangword_file = open('TUGAS_3/slangword/20190327_slangword.txt', \"r\").read() # Membuka file yang berisi kata slang dan kata formal dan mengkonversi kedalam string\n",
    "slangwords_text = slangword_file.split('\\n') # Memisahkan kata berdasarkan baris namun kata slang dan kata formal masih belum terpisah. output : (['slang:formal', ...])\n",
    "for slang in slangwords_text:\n",
    "    split_slang = slang.split(\":\") # Memisahkan semua kata slang dan kata formal berdasarkan \"titik dua (:)\"\n",
    "    slangwords[split_slang[0]] = split_slang[1] # Mapping semua kata slang dan kata formal ke dalam dictionary. Output : {'slang' : 'formal', ...}\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['user_id'] = list(map(lambda tweet: tweet['id_str'], tweets_data))\n",
    "tweets['user_name'] = list(map(lambda tweet: tweet['user']['name'], tweets_data))\n",
    "tweets['user_screen_name'] = list(map(lambda tweet: tweet['user']['screen_name'], tweets_data))\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['address'] = list(map(lambda tweet: tweet['place']['full_name'] if tweet['place'] != None else None, tweets_data))\n",
    "hashtags = list(map(lambda tweet: tweet['entities']['hashtags'] if tweet['entities'] != None else None, tweets_data))\n",
    "tweets['hashtags'] = list(map(lambda tweet : ', '.join(list(map(lambda tw : tw['text'], tweet))), hashtags))\n",
    "mentions = list(map(lambda tweet: tweet['entities']['user_mentions'] if tweet['entities'] != None else None, tweets_data))\n",
    "tweets['mentions'] = list(map(lambda tweet : ', '.join(list(map(lambda tw : tw['screen_name'], tweet))), mentions))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweets['created_at'] = list(map(lambda tweet: tweet['created_at'], tweets_data))\n",
    "tweets['country'] = list(map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweets_data))\n",
    "#tweets.to_csv('export_dataframe.csv', encoding='utf-8')\n",
    "#tweets.to_excel('export_dataframe.xlsx')\n",
    "print (\"Jumlah data tweets :\", len(tweets))\n",
    "tweetsIn = tweets[tweets.lang == 'in']\n",
    "tweetsIn.head()\n",
    "print (\"Jumlah data tweets bahasa Indonesia :\", len(tweetsIn))\n",
    "\n",
    "stopword_file1 = open('TUGAS_3/stopword/20190327_stopword_id.txt', \"r\").read() # Membuka file stopword bahasa indonesia dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file2 = open('TUGAS_3/stopword_en/20190327_stopwords_en.txt', \"r\").read()  # Membuka file stopword bahasa inggris dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file3 = open('TUGAS_3/stopword_noise/20190327_stopword_noise.txt', \"r\").read()  # Membuka file stopword noise dan menjadikan isi file tersebut sebagai string\n",
    "stopword_file_all = stopword_file1 + stopword_file2 + stopword_file3  # Menggabungkan ketiga string stopword sebelumnya kedalam satu string\n",
    "stopwords = stopword_file_all.split('\\n') # Memisahkan kata dalam string yang sudah digambungkan berdasarkan baris\n",
    "\n",
    "slangwords = dict() # Membuat dictionary kosong untuk menyimpan kata slang dan formal sebagai key dan value\n",
    "slangwords_dataframe = pd.read_csv('colloquial-indonesian-lexicon.csv') # Membuka file csv yang berisi kata slang dan formal dan mengkonversi kedalam dataframe\n",
    "\n",
    "for slang, formal in zip(slangwords_dataframe['slang'], slangwords_dataframe['formal']):\n",
    "    slangwords[slang] = formal # Mapping kata slang dan formal dan memasukkan ke dalam dictionary secara berulang\n",
    "\n",
    "slangword_file = open('TUGAS_3/slangword/20190327_slangword.txt', \"r\").read() # Membuka file yang berisi kata slang dan kata formal dan mengkonversi kedalam string\n",
    "slangwords_text = slangword_file.split('\\n') # Memisahkan kata berdasarkan baris namun kata slang dan kata formal masih belum terpisah. output : (['slang:formal', ...])\n",
    "\n",
    "for slang in slangwords_text:\n",
    "    split_slang = slang.split(\":\") # Memisahkan semua kata slang dan kata formal berdasarkan \"titik dua (:)\"\n",
    "    slangwords[split_slang[0]] = split_slang[1] # Mapping semua kata slang dan kata formal ke dalam dictionary. Output : {'slang' : 'formal', ...}\n",
    "\n",
    "array_text = list(map(lambda tweet: unidecode(tweet).lower(), tweets['text'])) # Mapping semua text twitter dan memasukan ke dalam list. Output : ['teks panjang ...', 'teks panjang', ...]\n",
    "long_text = ' '.join(array_text) # Menggabungkan semua text yang berada dalam list kedalam satu text\n",
    "tokenized = tw_tokenizer().tokenize(long_text) # Memisahkan kata dalam text berasarkan \"spasi\"\n",
    "tokenized2 = [w for w in tokenized if w.isalnum()] # Filtering kata yang hanya berisi karakater a-z dan 0-9\n",
    "tokenized3 = list(map(lambda w : slangwords[w] if w in slangwords.keys() else w, tokenized2)) # Mengubah kata slang menjadi kata formal (kata slang dan kata formal yang diperoleh dari dictionary) \n",
    "#tokenized2 = list(map(lambda w : slangwords[w] if w in slangwords.keys() else None, tokenized))\n",
    "# print(tokenized2)\n",
    "# for w in tokenized2:\n",
    "#     if w in slangwords.keys():\n",
    "#         print(slangwords[w])\n",
    "# print(\"Slang word sudah diganti kata baku\")\n",
    "# print(tokenized3)\n",
    "\n",
    "# print(slangwords['yg'])\n",
    "remove_stopwords = [w for w in tokenized3 if w not in stopwords] # Filtering data dengan menghapus kata yang tidak bermakna (Stopword yang diperoleh dari file)\n",
    "Counter  = Counter(remove_stopwords) # Menghitung frekuensi muncul semua kata\n",
    "most_words = Counter.most_common(100) # Menghitung 100 kata yang paling banyak muncul (Tidak termasuk kata yang tidak bermakna)\n",
    "print(most_words)\n",
    "\n",
    "print(\"Operasi selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
